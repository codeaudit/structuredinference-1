{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Sampling 2D pose DKF trained on (any) 2D pose database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The next few variables are important for choosing *which* dataset/model should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "DS = 'mpii-ca2'\n",
    "\n",
    "if DS == 'ikeadb':\n",
    "    DS_DIR = './expt-ikeadb/'\n",
    "    PFX = DS_DIR + 'chkpt-ikeadb/'\n",
    "    CONFIG_PATH = PFX + 'DKF_lr-8_0000e-04-vm-L-inf-structured-dh-50-ds-10-nl-relu-bs-20-ep-2000-rs-600-ttype-simple_gated-etype-mlp-previnp-False-ar-1_0000e+01-rv-5_0000e-02-nade-False-nt-5000-cond-True-ikeadb-acts-config.pkl'\n",
    "    WEIGHT_PATH = PFX + 'DKF_lr-8_0000e-04-vm-L-inf-structured-dh-50-ds-10-nl-relu-bs-20-ep-2000-rs-600-ttype-simple_gated-etype-mlp-previnp-False-ar-1_0000e+01-rv-5_0000e-02-nade-False-nt-5000-cond-True-ikeadb-acts-EP425-params.npz'\n",
    "    EXTRA_ARGS = '-vm L -cond -infm structured -ds 10 -dh 50 -uid past-only'.split()\n",
    "elif DS == 'penn':\n",
    "    DS_DIR = './expt-penn-action/'\n",
    "    PFX = DS_DIR + 'chkpt-penn/'\n",
    "    CONFIG_PATH = PFX + 'DKF_lr-8_0000e-04-vm-L-inf-structured-dh-50-ds-10-nl-relu-bs-20-ep-2000-rs-600-ttype-simple_gated-etype-mlp-previnp-False-ar-1_0000e+01-rv-5_0000e-02-nade-False-nt-5000-cond-False-penn-acts-config.pkl'\n",
    "    WEIGHT_PATH = PFX + 'DKF_lr-8_0000e-04-vm-L-inf-structured-dh-50-ds-10-nl-relu-bs-20-ep-2000-rs-600-ttype-simple_gated-etype-mlp-previnp-False-ar-1_0000e+01-rv-5_0000e-02-nade-False-nt-5000-cond-False-penn-acts-EP1975-params.npz'\n",
    "    EXTRA_ARGS = '-vm L -infm structured -ds 10 -dh 50 -uid penn-acts'.split()\n",
    "elif DS == 'mpii-ca2':\n",
    "    DS_DIR = './expt-mpii-ca2/'\n",
    "    PFX = DS_DIR + 'chkpt-mpii-ca2/'\n",
    "    CONFIG_PATH = PFX + 'DKF_lr-8_0000e-04-vm-L-inf-structured-dh-50-ds-10-nl-relu-bs-20-ep-2000-rs-600-ttype-simple_gated-etype-mlp-previnp-False-ar-1_0000e+01-rv-5_0000e-02-nade-False-nt-5000-cond-False-mpii-ca2-config.pkl'\n",
    "    WEIGHT_PATH = PFX + 'DKF_lr-8_0000e-04-vm-L-inf-structured-dh-50-ds-10-nl-relu-bs-20-ep-2000-rs-600-ttype-simple_gated-etype-mlp-previnp-False-ar-1_0000e+01-rv-5_0000e-02-nade-False-nt-5000-cond-False-mpii-ca2-EP975-params.npz'\n",
    "    EXTRA_ARGS = '-vm L -infm structured -ds 10 -dh 50 -uid mpii-ca2'.split()\n",
    "else:\n",
    "    raise ValueError('Unknown dataset %s' % DS)\n",
    "\n",
    "import sys\n",
    "sys.path.append(DS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import addpaths\n",
    "from load import loadDataset\n",
    "import p2d_loader\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.signal import convolve2d\n",
    "import h5py\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "del sys.argv[1:]\n",
    "\n",
    "sys.argv.extend(EXTRA_ARGS)\n",
    "sys.argv.extend(['-reload', WEIGHT_PATH, '-params', CONFIG_PATH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "old_dir = os.getcwd()\n",
    "try:\n",
    "    os.chdir(DS_DIR)\n",
    "    dataset = loadDataset()\n",
    "finally:\n",
    "    os.chdir(old_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_poses = dataset['train']\n",
    "train_poses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from parse_args_dkf import params\n",
    "from utils.misc import removeIfExists,createIfAbsent,mapPrint,saveHDF5,displayTime\n",
    "from stinfmodel_fast.dkf import DKF\n",
    "import stinfmodel_fast.learning as DKF_learn\n",
    "import stinfmodel_fast.evaluate as DKF_evaluate\n",
    "from theano import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if 'p2d_action_names' in dataset:\n",
    "    act_names = dataset['p2d_action_names']\n",
    "    for idx, name in enumerate(act_names):\n",
    "        print('ID% 3i (action% 3i/%i): %s' % (idx, idx+1, len(act_names), name))\n",
    "    one_hot_acts = {}\n",
    "    hot_vec_size = len(act_names)\n",
    "    for hot_bit, name in enumerate(act_names):\n",
    "        one_hot_acts[name] = (np.arange(hot_vec_size) == hot_bit)\n",
    "parents = dataset['p2d_parents']\n",
    "print('Parents array: %s' % parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_cond = bool(params.get('use_cond', False))\n",
    "params['savedir']+=PFX\n",
    "\n",
    "# Add dataset and NADE parameters to \"params\" which will become part of the\n",
    "# model\n",
    "for k in ['dim_observations','data_type']:\n",
    "    params[k] = dataset[k]\n",
    "mapPrint('Options: ',params)\n",
    "if params['use_nade']:\n",
    "    params['data_type']='real_nade'\n",
    "\n",
    "# Remove from params\n",
    "removeIfExists('./NOSUCHFILE')\n",
    "reloadFile = params.pop('reloadFile')\n",
    "pfile=params.pop('paramFile')\n",
    "# paramFile is set inside the BaseClass in theanomodels\n",
    "# to point to the pickle file containing params\"\"\"\n",
    "assert os.path.exists(pfile),pfile+' not found. Need paramfile'\n",
    "print 'Reloading trained model from : ',reloadFile\n",
    "print 'Assuming ',pfile,' corresponds to model'\n",
    "dkf  = DKF(params, paramFile = pfile, reloadFile = reloadFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def smooth_seq(seq):\n",
    "    assert seq.ndim in {2, 3}, seq.shape\n",
    "    if seq.ndim == 3:\n",
    "        rv = np.zeros_like(seq)\n",
    "        for r in range(len(seq)):\n",
    "            rv[r] = p2d_loader.gauss_filter(seq[r], sigma=1.0)\n",
    "        return rv\n",
    "    # 2d, filter whole thing\n",
    "    return p2d_loader.gauss_filter(seq, sigma=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Non-action-conditional modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def scrape_any(poses, count, sigma, mu, parents):\n",
    "    indices = np.random.permutation(len(poses))[:count]\n",
    "    out_data = poses[indices]\n",
    "    return p2d_loader.reconstruct_poses(out_data * sigma + mu, parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not use_cond:\n",
    "    # No need to do conditional nonsense!\n",
    "    num_seqs = 32\n",
    "    seq_length = 256\n",
    "    dest_dir = DS_DIR + 'generated/'\n",
    "    try:\n",
    "        os.makedirs(dest_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    \n",
    "    oodles_of_samples = dkf.sample(nsamples=num_seqs, T=seq_length)\n",
    "    sample_X, sample_Z = oodles_of_samples\n",
    "    mu = dataset['p2d_mean'].reshape((1, 1, -1))\n",
    "    sigma = dataset['p2d_std'].reshape((1, 1, -1))\n",
    "    real_X = p2d_loader.reconstruct_poses(sample_X * sigma + mu, parents)\n",
    "        \n",
    "    # Scrape some training poses, too\n",
    "    train_poses = scrape_any(\n",
    "        dataset['train'],\n",
    "        num_seqs,\n",
    "        sigma,\n",
    "        mu,\n",
    "        parents)\n",
    "    val_poses = scrape_any(\n",
    "        dataset['valid'],\n",
    "        num_seqs,\n",
    "        sigma,\n",
    "        mu,\n",
    "        parents)\n",
    "        \n",
    "    smooth_sampled_times = smooth_seq(\n",
    "        real_X.reshape(real_X.shape[:2] + (-1,))\n",
    "    ).reshape(real_X.shape[:2] + (2, -1))\n",
    "    dest_fn = os.path.join(dest_dir, 'generated.npz')\n",
    "    print('Saving ' + dest_fn)\n",
    "    np.savez(\n",
    "        dest_fn, poses_gen=real_X,\n",
    "        poses_smooth=smooth_sampled_times,\n",
    "        poses_train=train_poses,\n",
    "        poses_val=val_poses,\n",
    "        parents=parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Action-conditional modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def scrape_by_act(poses, actions, one_hot_rep, count, sigma, mu, parents):\n",
    "    # actions should be N*T*A\n",
    "    # from IPython.core.debugger import Tracer; Tracer()()\n",
    "    assert np.prod(one_hot_rep.shape) == one_hot_rep.size, \\\n",
    "        \"one-hot rep must be a vector\"\n",
    "    act_num = np.argmax(one_hot_rep.flatten())\n",
    "    assert actions.ndim == 3, actions.shape\n",
    "    act_nums = np.argmax(actions, axis=2)\n",
    "    assert act_nums.shape == poses.shape[:2], \\\n",
    "        \"mismatched action shape %s and pose shape %s\" \\\n",
    "            % (actions.shape, poses.shape)\n",
    "    # try to find sequences that feature part of the action\n",
    "    has_act, = np.nonzero((act_num == act_nums).any(axis=1))\n",
    "    indices = has_act[np.random.permutation(len(has_act))][:count]\n",
    "    if len(indices) < count:\n",
    "        print('Only found %d instances of action with ID %d'\n",
    "              % (len(indices), act_num))\n",
    "    out_data = poses[indices]\n",
    "    return p2d_loader.reconstruct_poses(out_data * sigma + mu, parents)\n",
    "\n",
    "def sanitise_name(name):\n",
    "    # for sanitising filenames\n",
    "    return re.sub(r'[^a-z0-9_-]+', '-', name.lower()).strip('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if use_cond:\n",
    "    seqs_per_act = 9\n",
    "    seq_length = 256\n",
    "    dest_dir = DS_DIR + 'generated-wacts/'\n",
    "    try:\n",
    "        os.makedirs(dest_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "        \n",
    "    # start by generating some sequences for each action type\n",
    "    for act_name, one_hot_rep in one_hot_acts.items():\n",
    "        print('Working on action %s' % act_name)\n",
    "        U = np.stack([one_hot_rep] * seq_length, axis=0)\n",
    "        oodles_of_samples = dkf.sample(nsamples=seqs_per_act, T=seq_length, U=U)\n",
    "        sample_X, sample_Z = oodles_of_samples\n",
    "        mu = dataset['p2d_mean'].reshape((1, 1, -1))\n",
    "        sigma = dataset['p2d_std'].reshape((1, 1, -1))\n",
    "        real_X = p2d_loader.reconstruct_poses(sample_X * sigma + mu, parents)\n",
    "        \n",
    "        # Scrape some training poses, too\n",
    "        train_poses = scrape_by_act(\n",
    "            dataset['train'],\n",
    "            dataset['train_cond_vals'],\n",
    "            one_hot_rep,\n",
    "            seqs_per_act,\n",
    "            sigma,\n",
    "            mu,\n",
    "            parents)\n",
    "        val_poses = scrape_by_act(\n",
    "            dataset['valid'],\n",
    "            dataset['val_cond_vals'],\n",
    "            one_hot_rep,\n",
    "            seqs_per_act,\n",
    "            sigma,\n",
    "            mu,\n",
    "            parents)\n",
    "        \n",
    "        smooth_sampled_times = smooth_seq(\n",
    "            real_X.reshape(real_X.shape[:2] + (16,))\n",
    "        ).reshape(real_X.shape[:2] + (2, 8))\n",
    "        actn = sanitise_name(act_name)\n",
    "        dest_pfx = os.path.join(dest_dir, 'act-%s' % actn)\n",
    "        dest_fn = dest_pfx + '.npz'\n",
    "        print('Saving ' + dest_fn)\n",
    "        np.savez(\n",
    "            dest_fn, poses_gen=real_X,\n",
    "            poses_smooth=smooth_sampled_times,\n",
    "            poses_train=train_poses,\n",
    "            poses_val=val_poses,\n",
    "            parents=parents\n",
    "        )\n",
    "\n",
    "    # now choose random pairs of (distinct) actions and simulate\n",
    "    # a transition at half-way point\n",
    "    num_pairs = 10\n",
    "    nacts = len(act_names)\n",
    "    chosen_idxs = np.random.permutation(nacts * (nacts-1))[:num_pairs]\n",
    "    act_pairs = [(act_names[idxp%nacts], act_names[idxp//nacts]) \\\n",
    "                 for idxp in chosen_idxs]\n",
    "    for act1, act2 in act_pairs:\n",
    "        print('Computing sequence for action %s -> %s' % (act1, act2))\n",
    "        \n",
    "        len1 = seq_length // 2\n",
    "        len2 = seq_length - len1\n",
    "        rep1 = one_hot_acts[act1]\n",
    "        rep2 = one_hot_acts[act2]\n",
    "        U = np.stack([rep1] * len1 + [rep2] * len2, axis=0)\n",
    "        oodles_of_samples = dkf.sample(nsamples=seqs_per_act, T=seq_length, U=U)\n",
    "        sample_X, sample_Z = oodles_of_samples\n",
    "        mu = dataset['p2d_mean'].reshape((1, 1, -1))\n",
    "        sigma = dataset['p2d_std'].reshape((1, 1, -1))\n",
    "        real_X = p2d_loader.reconstruct_poses(sample_X * sigma + mu, parents)\n",
    "        \n",
    "        smooth_sampled_times = smooth_seq(\n",
    "            real_X.reshape(real_X.shape[:2] + (16,))\n",
    "        ).reshape(real_X.shape[:2] + (2, 8))\n",
    "        act1n = sanitise_name(act1)\n",
    "        act2n = sanitise_name(act2)\n",
    "        dest_pfx = os.path.join(\n",
    "            dest_dir,\n",
    "            'trans-%s-to-%s' % (act1n, act2n))\n",
    "        dest_fn = dest_pfx + '.npz'\n",
    "        print('Saving ' + dest_fn)\n",
    "        np.savez(\n",
    "            dest_fn,\n",
    "            poses_trans=real_X,\n",
    "            poses_trans_smooth=smooth_sampled_times,\n",
    "            parents=parents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Action classification\n",
    "\n",
    "The next section of the notebook will be dedicated to action classification using the pose DKF. Rougly, the setup is this:\n",
    "    \n",
    "1. Extract a sequences of poses $k$ poses, $p_{t:t+k-1}$, associated with a given action. I will try to make sure that $k \\geq 10$ (or that $k$ is otherwise suitably long).\n",
    "2. Run the inference network over $p_{t:t+k-1}$ to obtain $z_{t:t+k-1}$.\n",
    "3. Pass a pooled representation of the latent vectors into an SVM for classification (e.g. mean of values).\n",
    "\n",
    "Some fine points:\n",
    "\n",
    "- The current model is trained to require actions as input at each step of the encoder *and* decoder. Given that actions must be supplied to the network at each time step, it's not clear what an action classifier on top of the latents actually achieves. Here are some possible ways of dealing with that:\n",
    "   1. Ignore it. See if the SVM can recover the given action labels from latents alone. Doing so isn't terribly impressive, but *failing* to do so would be strong evidence that this is a poor approach to action classification (**what I'm doing right now**; I'll do something more intelligent for Penn and MPII).\n",
    "   2. Give the network random actions so as not to bias it. Unfortunately, the network isn't actually trained on random actions, so this could make the results meaningless.\n",
    "   3. Train a new model where the decoder is not action-conditional. The encoder could be action-conditional or non-action-conditional; again, it's unclear whether this will improve or harm classification accuracy when using latents.\n",
    "- The whole pipeline is likely to be sensitive to the latent pooling method used. Will have to experiment with picking the last latent, as well as mean/sum/max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def seq_latents(feat_sequence, true_action=None):\n",
    "    # must be (T, D)\n",
    "    assert feat_sequence.ndim == 2, feat_sequence.shape\n",
    "    cond_vals = None\n",
    "    if use_cond:\n",
    "        assert true_action is not None, \\\n",
    "            'need true action because latents are action-conditional'\n",
    "        cond_vals = np.zeros((1, len(feat_sequence), len(act_names)), dtype='float32')\n",
    "        cond_vals[0, range(len(feat_sequence)), true_action] = 1\n",
    "    mask = np.ones((1, len(feat_sequence),))\n",
    "    # the zs are just samples\n",
    "    # maybe I should get more of them? just a matter of calling .infer twice\n",
    "    feat_in = feat_sequence[np.newaxis, ...].astype('float32')\n",
    "    z, mu, logcov = DKF_evaluate.infer(dkf, feat_sequence[np.newaxis, ...], mask, cond_vals=cond_vals)\n",
    "    # TODO: what if I condition on cat[mu, logcov] instead? Noise from z computation\n",
    "    # *might* act as a regulariser, but it might also just be noise :P\n",
    "    return z[0]\n",
    "    \n",
    "def to_latent_ds(ds):\n",
    "    \"\"\"Convert a list of (pose sequence, action ID) pairs into a dataset\n",
    "    consisting of a matrix of latents (corresponding to pose sequences)\n",
    "    and a vector of action IDs (as supplied).\"\"\"\n",
    "    X_blocks = []\n",
    "    Y_blocks = []\n",
    "    seen = 0\n",
    "    for feat_seq, true_act in ds:\n",
    "        if true_act == 0 and DS == 'ikeadb':\n",
    "            # action 0 on IkeaDB just means \"unlabelled\"\n",
    "            # XXX: how the hell is this continuing to happen?! I'm still getting action 0.\n",
    "            continue\n",
    "        seen += 1\n",
    "        if seen == 1 or seen % 250 == 0:\n",
    "            print('Working on sequence %d' % seen)\n",
    "        latents = seq_latents(feat_seq, true_act)\n",
    "        # take the mean\n",
    "        # lat_val = latents.mean(axis=0)\n",
    "        # take the last\n",
    "        lat_val = latents[-1]\n",
    "        X_blocks.append(lat_val)\n",
    "        Y_blocks.append(true_act)\n",
    "\n",
    "    X = np.stack(X_blocks)\n",
    "    Y = np.array(Y_blocks)\n",
    "    assert X.ndim == 2, X.shape\n",
    "    assert Y.ndim == 1, Y.shape\n",
    "    assert X.shape[0] ==  Y.shape[0], (X.shape, Y.shape)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def balance_aclass_ds(aclass_ds, act_names):\n",
    "    # find appropriate number of samples for a single action class,\n",
    "    # then trim \"heavy\" action classes to have no more than\n",
    "    # that number of samples\n",
    "    class_map = np.zeros((len(aclass_ds), len(act_names)))\n",
    "    for ds_idx, item in enumerate(aclass_ds):\n",
    "        _, class_num = item\n",
    "        class_map[ds_idx, class_num] = 1\n",
    "    support = class_map.sum(axis=0)\n",
    "    support_target = int(np.min(support))\n",
    "    to_keep = np.zeros((len(aclass_ds),))\n",
    "    for class_num in range(len(act_names)):\n",
    "        if support[class_num] <= support_target:\n",
    "            to_keep[class_map[:, class_num] == 1] = 1\n",
    "        else:\n",
    "            # drop all but [:median_support] of these\n",
    "            class_inds, = np.nonzero(class_map[:, class_num])\n",
    "            perm = np.random.permutation(len(class_inds))[:support_target]\n",
    "            chosen_inds = class_inds[perm]\n",
    "            to_keep[chosen_inds] = 1\n",
    "    rv = []\n",
    "    for choose_ind in np.nonzero(to_keep)[0]:\n",
    "        rv.append(aclass_ds[choose_ind])\n",
    "    return rv\n",
    "\n",
    "def merge_actions(aclass_ds, merge_map):\n",
    "    for class_name in act_names:\n",
    "        if class_name not in merge_map:\n",
    "            merge_map[class_name] = class_name\n",
    "    new_class_names = sorted({\n",
    "        class_name for class_name in merge_map.values()\n",
    "        if class_name is not None\n",
    "    })\n",
    "    new_class_nums = []\n",
    "    for class_name in act_names:\n",
    "        new_name = merge_map[class_name]\n",
    "        if new_name is None:\n",
    "            new_num = None\n",
    "        else:\n",
    "            new_num = new_class_names.index(new_name)\n",
    "        new_class_nums.append(new_num)\n",
    "    new_aclass_ds = []\n",
    "    for poses, action in aclass_ds:\n",
    "        new_action = new_class_nums[action]\n",
    "        if new_action is None:\n",
    "            continue\n",
    "        new_aclass_ds.append((poses, new_action))\n",
    "    return new_class_names, new_aclass_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The next cell is slooooow. Could probably speed it up with batching, but IMO not worth the effort at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if 'p2d_action_names' in dataset:\n",
    "    train_aclass_ds = dataset['train_aclass_ds']\n",
    "    val_aclass_ds = dataset['val_aclass_ds']\n",
    "    aclass_target_names = act_names\n",
    "    if DS == 'ikeadb':\n",
    "        merge_map = {\n",
    "            'attach leg 1': 'attach leg',\n",
    "            'attach leg 2': 'attach leg',\n",
    "            'attach leg 3': 'attach leg',\n",
    "            'attach leg 4': 'attach leg',\n",
    "            'detach leg 1': 'detach leg',\n",
    "            'detach leg 2': 'detach leg',\n",
    "            'detach leg 3': 'detach leg',\n",
    "            'detach leg 4': 'detach leg',\n",
    "            'n/a': None\n",
    "        }\n",
    "        _, train_aclass_ds \\\n",
    "            = merge_actions(train_aclass_ds, merge_map)\n",
    "        aclass_target_names, val_aclass_ds \\\n",
    "            = merge_actions(val_aclass_ds, merge_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if 'p2d_action_names' in dataset:\n",
    "    train_aclass_ds_bal = balance_aclass_ds(train_aclass_ds, aclass_target_names)\n",
    "    train_act_X, train_act_Y = to_latent_ds(train_aclass_ds_bal)\n",
    "    val_aclass_ds_bal = balance_aclass_ds(val_aclass_ds, aclass_target_names)\n",
    "    val_act_X, val_act_Y = to_latent_ds(val_aclass_ds_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if 'p2d_action_names' in dataset:\n",
    "    print('Shapes:')\n",
    "    print('train_act_X: {}'.format(train_act_X.shape))\n",
    "    print('train_act_Y: {}'.format(train_act_Y.shape))\n",
    "    print('val_act_X: {}'.format(val_act_X.shape))\n",
    "    print('val_act_Y: {}'.format(val_act_Y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if 'p2d_action_names' in dataset:\n",
    "    model = LinearSVC(C=1)\n",
    "    print('Fitting to training set')\n",
    "    model.fit(train_act_X, train_act_Y)\n",
    "    print('Evaluating SVC on training set')\n",
    "    train_out_Y = model.predict(train_act_X)\n",
    "    print(classification_report(train_act_Y, train_out_Y,\n",
    "                                target_names=aclass_target_names))\n",
    "\n",
    "    print('Evaluating SVC on validation set')\n",
    "    val_out_Y = model.predict(val_act_X)\n",
    "    print(classification_report(val_act_Y, val_out_Y,\n",
    "                                target_names=aclass_target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Inspecting predictions\n",
    "\n",
    "How well is the model actually learning to predict pose sequences? The next section will try to find out by taking short pose sequences from the training set (any action), encoding them, then coming up with several predictions for their continuations (both smoothed and unsmoothed). Eventually, I will superimpose the predictions on the original frames corresponding to the poses for comparison with the ground truth. Obviously, this will require that I output the ground truth as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_completion(poses):\n",
    "    T = len(poses)\n",
    "    T_pre = T // 2\n",
    "    T_post = T - T_pre\n",
    "    \n",
    "    prior_completion = np.zeros_like(poses)\n",
    "    posterior_completion = np.zeros_like(poses)\n",
    "    \n",
    "    pre_latents = seq_latents(poses[:T_pre])\n",
    "    prior_completion[:T_pre] \\\n",
    "        = posterior_completion[:T_pre] \\\n",
    "        = dkf.emission_fxn(pre_latents[None, ...])\n",
    "        \n",
    "    z = pre_latents[-1]\n",
    "    assert z.ndim == 1\n",
    "    # z needs to be 3D (nsamples, time, stochdim)\n",
    "    z_prior = z_posterior = z.reshape((1, 1, -1))\n",
    "    \n",
    "    for t in range(T_pre, T):\n",
    "        # completion based on prior only\n",
    "        mu_prior, logcov_prior = dkf.transition_fxn(z_prior)\n",
    "        z_prior = DKF_evaluate.sampleGaussian(mu_prior, logcov_prior).astype(config.floatX)\n",
    "        e_prior = dkf.emission_fxn(z_prior)\n",
    "        assert e_prior.ndim == 3 and e_prior.shape[:2] == (1, 1)\n",
    "        prior_completion[t] = e_prior[0][0]\n",
    "        \n",
    "        # posterior completion (TODO: use stateful model to make this faster)\n",
    "        mu_posterior, logcov_posterior = dkf.transition_fxn(z_posterior)\n",
    "        # sample z_t from prior distribution to gt x_t\n",
    "        z_posterior = DKF_evaluate.sampleGaussian(mu_posterior, logcov_posterior).astype(config.floatX)\n",
    "        e_posterior = dkf.emission_fxn(z_posterior)\n",
    "        assert e_prior.ndim == 3 and e_posterior.shape[:2] == (1, 1)\n",
    "        posterior_completion[t] = e_posterior[0][0]\n",
    "        # here's the trick: we replace our prior z_t with a posterior\n",
    "        # z_t now that we have x\n",
    "        z_posterior = seq_latents(posterior_completion[:t])[None, t-1:]\n",
    "        assert z_posterior.ndim == 3\n",
    "        assert z_posterior.shape[:2] == (1, 1)\n",
    "\n",
    "    return prior_completion, posterior_completion, T_pre\n",
    "\n",
    "def process_completions(completions, to_select=32):\n",
    "    right_indices = np.random.permutation(len(completions))[:to_select]\n",
    "    \n",
    "    rv = []\n",
    "    \n",
    "    for block in np.take(completions, right_indices):\n",
    "        poses = block['poses']\n",
    "        mask = block['mask']\n",
    "        completion_p, completion_q, crossover = make_completion(poses)\n",
    "        start, stop, skip = block['start'], block['stop'], block['skip']\n",
    "        # need to undo mean subtraction and parent-relative stuff\n",
    "        # to reconstruct poses\n",
    "        # TODO: what if this is Penn or something and I'm not using\n",
    "        # parent-relative parameterisation?\n",
    "        recon = lambda data: p2d_loader.reconstruct_poses(data * sigma + mu, parents).tolist()\n",
    "        \n",
    "        completed_block = {\n",
    "            'vid_name': block['vid_name'],\n",
    "            # I think these indices will be zero-based\n",
    "            # (but datasets are not)\n",
    "            'frame_inds': list(range(start, stop, skip)),\n",
    "            'true_poses': recon(poses),\n",
    "            'prior_poses': recon(completion_p),\n",
    "            'posterior_poses': recon(completion_q),\n",
    "            # time at which we go from using emission function \n",
    "            # on posterior latents to chaining prior evaluation\n",
    "            # + using posterior on that\n",
    "            'crossover_time': crossover,\n",
    "            'mask': (mask != 0).tolist()\n",
    "        }\n",
    "        \n",
    "        rv.append(completed_block)\n",
    "    \n",
    "    return rv\n",
    "\n",
    "def save_completions(completions, save_subdir):\n",
    "    save_dir = os.path.join(DS_DIR, save_subdir)\n",
    "    try:\n",
    "        os.makedirs(save_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    for block_id, block in enumerate(completions):\n",
    "        file_path = os.path.join(save_dir, '%04d.json' % block_id)\n",
    "        with open(file_path, 'w') as fp:\n",
    "            json.dump(block, fp, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if 'p2d_train_completions' in dataset and not use_cond:\n",
    "    # TODO: figure out how to do this with action-conditional model\n",
    "    print('Working on train completions')\n",
    "    train_comp = process_completions(dataset['p2d_train_completions'])\n",
    "    save_completions(train_comp, 'completions/train')\n",
    "    \n",
    "    print('Working on val completions')\n",
    "    val_comp = process_completions(dataset['p2d_val_completions'])\n",
    "    save_completions(val_comp, 'completions/val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
